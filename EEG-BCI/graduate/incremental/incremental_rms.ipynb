{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "import seaborn as sns #绘制confusion matrix heatmap\n",
    "\n",
    "import os\n",
    "import scipy.io as sio\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "import tqdm\n",
    "import  time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore') #忽略警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 256\n",
    "origin_channel = 16\n",
    "\n",
    "\n",
    "SAMPLE_CHANNEL = ['Pz' , 'PO3' , 'PO4' , 'O1' , 'O2' , 'Oz' , 'O9' , 'FP2' ,\n",
    "                  'C4' , 'C6' , 'CP3' , 'CP1' ,\n",
    "                  'CPZ' , 'CP2' , 'CP4' , 'PO8']\n",
    "\n",
    "LABEL2STR = {0:'sen' , 1:'hong' , 2:'zhao',\n",
    "             3:'fen' , 4:'xiao' , 5:'yu' , \n",
    "             6:'bin' , 7:'wang' , 8:'wei' , \n",
    "             9:'fei'}\n",
    "\n",
    "CLIP_FORWARD = 1 #首部裁掉时间\n",
    "CLIP_BACKWARD = 1 #尾部裁掉时间\n",
    "\n",
    "trial_time = 3 #segment second\n",
    "\n",
    "\n",
    "#是否进行归一化\n",
    "#reference:a study on performance increasing in ssvep based bci application\n",
    "#IS_NORMALIZE = True\n",
    "\n",
    "#是否进行滤波\n",
    "#IS_FILTER = False\n",
    "#EEG频率范围\n",
    "#reference:a study on performance increasing in ssvep based bci application\n",
    "LO_FREQ = 0.5\n",
    "HI_FREQ = 40\n",
    "\n",
    "#是否陷波\n",
    "#IS_NOTCH = False\n",
    "NOTCH_FREQ = 50 #陷波 工频\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "\n",
    "    data = sio.loadmat(file_name=filename)['data_received'] #length*16 matrix\n",
    "\n",
    "    data = data[CLIP_FORWARD * sample_rate : - CLIP_BACKWARD * sample_rate] #首部 尾部 进行裁剪\n",
    "   \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(data , label , overlap_length):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    size = sample_rate * trial_time #一小段 256*3 个数据点\n",
    "    data_length = data.shape[0]\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    while idx<=data_length-size:\n",
    "        train_data.append(data[idx : idx+size , :])\n",
    "        train_labels.append(label)\n",
    "\n",
    "        idx = idx + (size - overlap_length)\n",
    "        \n",
    "    return np.array(train_data) , np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_t_v(filenames):\n",
    "    # np.random.shuffle(filenames)\n",
    "    \n",
    "    return np.random.choice(filenames , size=10) #20次的计算准确率中 每次随机选择10个样本进行训练测试\n",
    "\n",
    "def combine(freq):    \n",
    "    overlap_length = 2*256 #重叠2秒数据\n",
    "    \n",
    "    #保证随机性 进行置乱\n",
    "    person_0_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/0/' % freq) )\n",
    "    person_1_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/1/' % freq) )\n",
    "    person_2_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/2/' % freq) )\n",
    "    person_3_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/3/' % freq) )\n",
    "    person_4_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/4/' % freq) )\n",
    "    person_5_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/5/' % freq) )\n",
    "    person_6_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/6/' % freq) )\n",
    "    person_7_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/7/' % freq) )\n",
    "    person_8_filenames = shuffle_t_v( os.listdir('data/base_rf/%s/8/' % freq) )\n",
    "\n",
    "    #打开信号文件 并 合并\n",
    "    person_0 = np.concatenate([load_data('data/base_rf/%s/0/' % freq + filename) for filename in person_0_filenames] , axis = 0)\n",
    "    person_1 = np.concatenate([load_data('data/base_rf/%s/1/' % freq + filename) for filename in person_1_filenames] , axis = 0)\n",
    "    person_2 = np.concatenate([load_data('data/base_rf/%s/2/' % freq + filename) for filename in person_2_filenames] , axis = 0)\n",
    "    person_3 = np.concatenate([load_data('data/base_rf/%s/3/' % freq + filename) for filename in person_3_filenames] , axis = 0)\n",
    "    person_4 = np.concatenate([load_data('data/base_rf/%s/4/' % freq + filename) for filename in person_4_filenames] , axis = 0)\n",
    "    person_5 = np.concatenate([load_data('data/base_rf/%s/5/' % freq + filename) for filename in person_5_filenames] , axis = 0)\n",
    "    person_6 = np.concatenate([load_data('data/base_rf/%s/6/' % freq + filename) for filename in person_6_filenames] , axis = 0)\n",
    "    person_7 = np.concatenate([load_data('data/base_rf/%s/7/' % freq + filename) for filename in person_7_filenames] , axis = 0)\n",
    "    person_8 = np.concatenate([load_data('data/base_rf/%s/8/' % freq + filename) for filename in person_8_filenames] , axis = 0)\n",
    "    \n",
    "    #============\n",
    "    #训练数据分段\n",
    "    train_person_data_0 , train_person_labels_0 = separate(person_0 , label = 0 , overlap_length=overlap_length)\n",
    "    train_person_data_1 , train_person_labels_1 = separate(person_1 , label = 1 , overlap_length=overlap_length)\n",
    "    train_person_data_2 , train_person_labels_2 = separate(person_2 , label = 2 , overlap_length=overlap_length)\n",
    "    train_person_data_3 , train_person_labels_3 = separate(person_3 , label = 3 , overlap_length=overlap_length)\n",
    "    train_person_data_4 , train_person_labels_4 = separate(person_4 , label = 4 , overlap_length=overlap_length)\n",
    "    train_person_data_5 , train_person_labels_5 = separate(person_5 , label = 5 , overlap_length=overlap_length)\n",
    "    train_person_data_6 , train_person_labels_6 = separate(person_6 , label = 6 , overlap_length=overlap_length)\n",
    "    train_person_data_7 , train_person_labels_7 = separate(person_7 , label = 7 , overlap_length=overlap_length)\n",
    "    train_person_data_8 , train_person_labels_8 = separate(person_8 , label = 8 , overlap_length=overlap_length)\n",
    "\n",
    "    #合并数据\n",
    "    train_data = np.concatenate((train_person_data_0 , train_person_data_1 , train_person_data_2 ,\n",
    "                                 train_person_data_3 , train_person_data_4 , train_person_data_5 ,\n",
    "                                 train_person_data_6 , train_person_data_7 , train_person_data_8 ,\n",
    "                                 ))\n",
    "    \n",
    "    train_labels = np.concatenate((train_person_labels_0 , train_person_labels_1 , train_person_labels_2 ,\n",
    "                                   train_person_labels_3 , train_person_labels_4 , train_person_labels_5 ,\n",
    "                                   train_person_labels_6 , train_person_labels_7 , train_person_labels_8 ,\n",
    "                                    ))\n",
    "    \n",
    "    #产生索引并置乱\n",
    "    idx_train_data = list(range(train_data.shape[0]))\n",
    "    np.random.shuffle(idx_train_data)\n",
    "\n",
    "    #将训练数据置乱\n",
    "    train_data = train_data[idx_train_data]\n",
    "    train_labels = train_labels[idx_train_data]\n",
    "        \n",
    "    return train_data , train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_data_labels(session_id , freq , is_training):\n",
    "    if is_training:\n",
    "        overlap_length = 256*2\n",
    "    else:\n",
    "        overlap_length = 0\n",
    "    \n",
    "    str_freq = str(freq)\n",
    "    \n",
    "    subjcets = os.listdir('data/incremental/%s/s%d/' % (str_freq , session_id)) #受试者ID\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for subjcet in subjcets:\n",
    "        filenames = os.listdir('data/incremental/%s/s%d/%s/' % (str_freq , session_id , subjcet))\n",
    "        \n",
    "        person = np.concatenate([load_data('data/incremental/%s/s%d/%s/%s' % (str_freq , session_id , subjcet , filename)) for filename in filenames] , axis = 0)\n",
    "        \n",
    "        person_data , person_label = separate( person , label = int(subjcet) , overlap_length = overlap_length)\n",
    "        \n",
    "        data.append(person_data)\n",
    "        labels.append(person_label)\n",
    "    \n",
    "    #合并数据\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    #shuffle\n",
    "    idx_data = list(range(data.shape[0]))\n",
    "    np.random.shuffle(idx_data)\n",
    "\n",
    "    data = data[idx_data]\n",
    "    labels = labels[idx_data]\n",
    "    \n",
    "    return data , labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_RMS(data):\n",
    "    def rms(datum):\n",
    "        '''\n",
    "        :datum: 一段信号 shape : 768 * 16\n",
    "        '''\n",
    "        return [ np.sqrt(np.mean(np.square( d ))) for d in datum.T ]\n",
    "    \n",
    "    feature_rms = []\n",
    "    \n",
    "    for datum in data: \n",
    "        feature_rms.append(rms(datum))\n",
    "    \n",
    "    return np.array(feature_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_shuffle(orig_X , orig_y , session_id , freq):\n",
    "    session_id_data , session_id_labels = session_data_labels(session_id , freq , is_training=True)\n",
    "    session_id_data = feature_extraction_RMS(session_id_data)\n",
    "    # session_id_labels = to_categorical(session_id_labels , num_classes=9)\n",
    "    \n",
    "    orig_X = np.concatenate((orig_X , session_id_data) , axis=0)\n",
    "    orig_y = np.concatenate((orig_y , session_id_labels) , axis=0)\n",
    "    \n",
    "    idx = list(range(orig_X.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    orig_X = orig_X[idx]\n",
    "    orig_y = orig_y[idx]\n",
    "    \n",
    "    return orig_X , orig_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq =  6\n",
      "aver time :  [0.07709084999999724, 0.07709084999999724, 0.0954551250000005, 0.1033475700000082, 0.11233255500000325, 0.12434944000000314, 0.12388603000001197, 0.12346875000000282, 0.1306207500000081]\n",
      "aver accu :  [0.2538647342995169, 0.23454106280193235, 0.1468599033816425, 0.825, 0.6678743961352656, 0.527536231884058, 0.8231884057971015, 0.8004830917874397, 0.6805555555555556]\n",
      "freq =  7.5\n",
      "aver time :  [0.06841061500000194, 0.06841061500000194, 0.07648676000000591, 0.08863412999999128, 0.09785799999999653, 0.11240270499999952, 0.12705591499999686, 0.12403938000001062, 0.13167374000000506]\n",
      "aver accu :  [0.7086956521739131, 0.5516908212560387, 0.33671497584541055, 0.4398550724637681, 0.5968599033816424, 0.5221014492753623, 0.7543478260869565, 0.9118357487922705, 0.590096618357488]\n",
      "freq =  8.5\n",
      "aver time :  [0.07295142999996358, 0.07295142999996358, 0.08632505499996057, 0.09796234999993772, 0.09747609999992904, 0.09489111999988324, 0.0952502599999292, 0.10165756499989129, 0.1197039049998807]\n",
      "aver accu :  [0.23756038647342992, 0.37886473429951695, 0.4032608695652174, 0.743719806763285, 0.7259661835748792, 0.6884057971014492, 0.9066425120772946, 0.730072463768116, 0.6566425120772947]\n",
      "freq =  10\n",
      "aver time :  [0.07009129000001053, 0.07009129000001053, 0.07310723000009034, 0.08541170500014346, 0.07955700500015724, 0.08770218500021656, 0.07097227500024701, 0.0783436250002751, 0.08384342500036723]\n",
      "aver accu :  [0.2876811594202898, 0.2942028985507246, 0.2946859903381642, 0.5379227053140098, 0.7844202898550725, 0.8177536231884058, 1.0, 0.5696859903381644, 0.7751851851851852]\n"
     ]
    }
   ],
   "source": [
    "def cal_time(ss):\n",
    "    step = ss[1]-ss[0]\n",
    "    \n",
    "    for i in range(len(ss)):\n",
    "        ss[i]-=step*i\n",
    "    return ss\n",
    "\n",
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq = ' , freq)\n",
    "    \n",
    "    times = []\n",
    "    accus = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        times_sub = []\n",
    "        accus_sub = []\n",
    "    \n",
    "        train_X_ , train_y = session_data_labels(1 , freq , is_training=True)\n",
    "        #train_X_ , train_y = combine(freq)\n",
    "        train_X = feature_extraction_RMS(train_X_) #SBP特征提取\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=16)\n",
    "        rf.fit(train_X , train_y)\n",
    "\n",
    "        for idx , session_id in enumerate([3,5,6,7,8,9,11,12,13]):\n",
    "            session_N_data , session_N_labels = session_data_labels(session_id , freq , is_training=False)\n",
    "            session_N_data = feature_extraction_RMS(session_N_data)\n",
    "\n",
    "            start = time.clock()\n",
    "            score = rf.score(session_N_data , session_N_labels)\n",
    "            time1 = time.clock() - start\n",
    "            \n",
    "            accus_sub.append(score)\n",
    "\n",
    "            #print( freq , session_id , score)\n",
    "\n",
    "            #更新模型\n",
    "            train_X , train_y = concat_and_shuffle(train_X , train_y , session_id , freq)\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators=16)\n",
    "\n",
    "            start = time.clock()\n",
    "            rf.fit(train_X , train_y)\n",
    "            time2 = time.clock() - start\n",
    "\n",
    "            times_sub.append(time1 + time2)\n",
    "\n",
    "        times_sub = cal_time(times_sub)\n",
    "        #print(times_sub)\n",
    "        \n",
    "        times.append(times_sub)\n",
    "        accus.append(accus_sub)\n",
    "        \n",
    "    times = np.array(times)\n",
    "    accus = np.array(accus)\n",
    "    \n",
    "    print('aver time : ' , list(map(np.mean , times.T )) )\n",
    "    print('aver accu : ' , list(map(np.mean , accus.T )) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq =  6\n",
      "aver time :  [0.09469644000000699, 0.09469644000000699, 0.10862856500006046, 0.11268674000004922, 0.11681907000015598, 0.11736423500009323, 0.11301141000020606, 0.11716198000024178, 0.11945227500018518]\n",
      "aver accu :  [0.25555555555555554, 0.2335748792270531, 0.14480676328502418, 0.8252415458937199, 0.6695652173913043, 0.5478260869565218, 0.8143719806763287, 0.8249999999999998, 0.6634057971014492]\n",
      "freq =  7.5\n",
      "aver time :  [0.08928006500004812, 0.08928006500004812, 0.11123157499994249, 0.13329333999993195, 0.14163832000015192, 0.16757647000006273, 0.1809880249998514, 0.19692223499980627, 0.2247597199998154]\n",
      "aver accu :  [0.7022946859903382, 0.5566425120772946, 0.3425120772946859, 0.4346618357487923, 0.5835748792270532, 0.533816425120773, 0.7643719806763285, 0.918599033816425, 0.5804347826086957]\n",
      "freq =  8.5\n",
      "aver time :  [0.09044490999997379, 0.09044490999997379, 0.10545898999992005, 0.12021774499980893, 0.11514265499986323, 0.11551947999967069, 0.11826538999955574, 0.1292127499995786, 0.14829527999938819]\n",
      "aver accu :  [0.23055555555555554, 0.42113526570048315, 0.4202898550724637, 0.7542270531400964, 0.7239130434782608, 0.6857487922705313, 0.9016908212560386, 0.7497584541062803, 0.6665458937198068]\n",
      "freq =  10\n",
      "aver time :  [0.09250228500009143, 0.09250228500009143, 0.10345508000016253, 0.12320354500013764, 0.12462831500024549, 0.13848730000024717, 0.12057743500038214, 0.14392552500050898, 0.14407828000050812]\n",
      "aver accu :  [0.301207729468599, 0.2664251207729469, 0.3022946859903382, 0.5745169082125605, 0.7917874396135267, 0.8216183574879228, 1.0, 0.5695652173913044, 0.775925925925926]\n"
     ]
    }
   ],
   "source": [
    "def cal_time(ss):\n",
    "    step = ss[1]-ss[0]\n",
    "    \n",
    "    for i in range(len(ss)):\n",
    "        ss[i]-=step*i\n",
    "    return ss\n",
    "\n",
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq = ' , freq)\n",
    "    \n",
    "    times = []\n",
    "    accus = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        times_sub = []\n",
    "        accus_sub = []\n",
    "    \n",
    "        train_X_ , train_y = session_data_labels(1 , freq , is_training=True)\n",
    "        #train_X_ , train_y = combine(freq)\n",
    "        train_X = feature_extraction_RMS(train_X_) #SBP特征提取\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=20)\n",
    "        rf.fit(train_X , train_y)\n",
    "\n",
    "        for idx , session_id in enumerate([3,5,6,7,8,9,11,12,13]):\n",
    "            session_N_data , session_N_labels = session_data_labels(session_id , freq , is_training=False)\n",
    "            session_N_data = feature_extraction_RMS(session_N_data)\n",
    "\n",
    "            start = time.clock()\n",
    "            score = rf.score(session_N_data , session_N_labels)\n",
    "            time1 = time.clock() - start\n",
    "            \n",
    "            accus_sub.append(score)\n",
    "\n",
    "            #print( freq , session_id , score)\n",
    "\n",
    "            #更新模型\n",
    "            train_X , train_y = concat_and_shuffle(train_X , train_y , session_id , freq)\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "            start = time.clock()\n",
    "            rf.fit(train_X , train_y)\n",
    "            time2 = time.clock() - start\n",
    "\n",
    "            times_sub.append(time1 + time2)\n",
    "\n",
    "        times_sub = cal_time(times_sub)\n",
    "        #print(times_sub)\n",
    "        \n",
    "        times.append(times_sub)\n",
    "        accus.append(accus_sub)\n",
    "        \n",
    "    times = np.array(times)\n",
    "    accus = np.array(accus)\n",
    "    \n",
    "    print('aver time : ' , list(map(np.mean , times.T )) )\n",
    "    print('aver accu : ' , list(map(np.mean , accus.T )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
