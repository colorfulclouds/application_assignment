{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "import seaborn as sns #绘制confusion matrix heatmap\n",
    "\n",
    "import os\n",
    "import scipy.io as sio\n",
    "\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "import tqdm\n",
    "import  time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore') #忽略警告\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 256\n",
    "origin_channel = 16\n",
    "\n",
    "\n",
    "SAMPLE_CHANNEL = ['Pz' , 'PO3' , 'PO4' , 'O1' , 'O2' , 'Oz' , 'O9' , 'FP2' ,\n",
    "                  'C4' , 'C6' , 'CP3' , 'CP1' ,\n",
    "                  'CPZ' , 'CP2' , 'CP4' , 'PO8']\n",
    "\n",
    "LABEL2STR = {0:'sen' , 1:'hong' , 2:'zhao',\n",
    "             3:'fen' , 4:'xiao' , 5:'yu' , \n",
    "             6:'bin' , 7:'wang' , 8:'wei' , \n",
    "             9:'fei'}\n",
    "\n",
    "CLIP_FORWARD = 1 #首部裁掉时间\n",
    "CLIP_BACKWARD = 1 #尾部裁掉时间\n",
    "\n",
    "trial_time = 3 #segment second\n",
    "\n",
    "\n",
    "#是否进行归一化\n",
    "#reference:a study on performance increasing in ssvep based bci application\n",
    "#IS_NORMALIZE = True\n",
    "\n",
    "#是否进行滤波\n",
    "#IS_FILTER = False\n",
    "#EEG频率范围\n",
    "#reference:a study on performance increasing in ssvep based bci application\n",
    "LO_FREQ = 0.5\n",
    "HI_FREQ = 40\n",
    "\n",
    "#是否陷波\n",
    "#IS_NOTCH = False\n",
    "NOTCH_FREQ = 50 #陷波 工频\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "\n",
    "    data = sio.loadmat(file_name=filename)['data_received'] #length*16 matrix\n",
    "\n",
    "    data = data[CLIP_FORWARD * sample_rate : - CLIP_BACKWARD * sample_rate] #首部 尾部 进行裁剪\n",
    "   \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate(data , label , overlap_length):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    size = sample_rate * trial_time #一小段 256*3 个数据点\n",
    "    data_length = data.shape[0]\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    while idx<=data_length-size:\n",
    "        train_data.append(data[idx : idx+size , :])\n",
    "        train_labels.append(label)\n",
    "\n",
    "        idx = idx + (size - overlap_length)\n",
    "        \n",
    "    return np.array(train_data) , np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_t_v(filenames):\n",
    "    # np.random.shuffle(filenames)\n",
    "    \n",
    "    return np.random.choice(filenames , size=10) #20次的计算准确率中 每次随机选择10个样本进行训练测试\n",
    "\n",
    "def combine(freq):    \n",
    "    overlap_length = 2*256 #重叠2秒数据\n",
    "    \n",
    "    #保证随机性 进行置乱\n",
    "    person_0_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/0/' % freq) )\n",
    "    person_1_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/1/' % freq) )\n",
    "    person_2_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/2/' % freq) )\n",
    "    person_3_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/3/' % freq) )\n",
    "    person_4_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/4/' % freq) )\n",
    "    person_5_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/5/' % freq) )\n",
    "    person_6_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/6/' % freq) )\n",
    "    person_7_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/7/' % freq) )\n",
    "    person_8_filenames = shuffle_t_v( os.listdir('../incremental/data/base_rf/%s/8/' % freq) )\n",
    "\n",
    "    #打开信号文件 并 合并\n",
    "    person_0 = np.concatenate([load_data('../incremental/data/base_rf/%s/0/' % freq + filename) for filename in person_0_filenames] , axis = 0)\n",
    "    person_1 = np.concatenate([load_data('../incremental/data/base_rf/%s/1/' % freq + filename) for filename in person_1_filenames] , axis = 0)\n",
    "    person_2 = np.concatenate([load_data('../incremental/data/base_rf/%s/2/' % freq + filename) for filename in person_2_filenames] , axis = 0)\n",
    "    person_3 = np.concatenate([load_data('../incremental/data/base_rf/%s/3/' % freq + filename) for filename in person_3_filenames] , axis = 0)\n",
    "    person_4 = np.concatenate([load_data('../incremental/data/base_rf/%s/4/' % freq + filename) for filename in person_4_filenames] , axis = 0)\n",
    "    person_5 = np.concatenate([load_data('../incremental/data/base_rf/%s/5/' % freq + filename) for filename in person_5_filenames] , axis = 0)\n",
    "    person_6 = np.concatenate([load_data('../incremental/data/base_rf/%s/6/' % freq + filename) for filename in person_6_filenames] , axis = 0)\n",
    "    person_7 = np.concatenate([load_data('../incremental/data/base_rf/%s/7/' % freq + filename) for filename in person_7_filenames] , axis = 0)\n",
    "    person_8 = np.concatenate([load_data('../incremental/data/base_rf/%s/8/' % freq + filename) for filename in person_8_filenames] , axis = 0)\n",
    "    \n",
    "    #============\n",
    "    #训练数据分段\n",
    "    train_person_data_0 , train_person_labels_0 = separate(person_0 , label = 0 , overlap_length=overlap_length)\n",
    "    train_person_data_1 , train_person_labels_1 = separate(person_1 , label = 1 , overlap_length=overlap_length)\n",
    "    train_person_data_2 , train_person_labels_2 = separate(person_2 , label = 2 , overlap_length=overlap_length)\n",
    "    train_person_data_3 , train_person_labels_3 = separate(person_3 , label = 3 , overlap_length=overlap_length)\n",
    "    train_person_data_4 , train_person_labels_4 = separate(person_4 , label = 4 , overlap_length=overlap_length)\n",
    "    train_person_data_5 , train_person_labels_5 = separate(person_5 , label = 5 , overlap_length=overlap_length)\n",
    "    train_person_data_6 , train_person_labels_6 = separate(person_6 , label = 6 , overlap_length=overlap_length)\n",
    "    train_person_data_7 , train_person_labels_7 = separate(person_7 , label = 7 , overlap_length=overlap_length)\n",
    "    train_person_data_8 , train_person_labels_8 = separate(person_8 , label = 8 , overlap_length=overlap_length)\n",
    "\n",
    "    #合并数据\n",
    "    train_data = np.concatenate((train_person_data_0 , train_person_data_1 , train_person_data_2 ,\n",
    "                                 train_person_data_3 , train_person_data_4 , train_person_data_5 ,\n",
    "                                 train_person_data_6 , train_person_data_7 , train_person_data_8 ,\n",
    "                                 ))\n",
    "    \n",
    "    train_labels = np.concatenate((train_person_labels_0 , train_person_labels_1 , train_person_labels_2 ,\n",
    "                                   train_person_labels_3 , train_person_labels_4 , train_person_labels_5 ,\n",
    "                                   train_person_labels_6 , train_person_labels_7 , train_person_labels_8 ,\n",
    "                                    ))\n",
    "    \n",
    "    #产生索引并置乱\n",
    "    idx_train_data = list(range(train_data.shape[0]))\n",
    "    np.random.shuffle(idx_train_data)\n",
    "\n",
    "    #将训练数据置乱\n",
    "    train_data = train_data[idx_train_data]\n",
    "    train_labels = train_labels[idx_train_data]\n",
    "        \n",
    "    return train_data , train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_data_labels(session_id , freq , is_training):\n",
    "    if is_training:\n",
    "        overlap_length = 256*2\n",
    "    else:\n",
    "        overlap_length = 0\n",
    "    \n",
    "    str_freq = str(freq)\n",
    "    \n",
    "    subjcets = os.listdir('../incremental/data/incremental/%s/s%d/' % (str_freq , session_id)) #受试者ID\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for subjcet in subjcets:\n",
    "        filenames = os.listdir('../incremental/data/incremental/%s/s%d/%s/' % (str_freq , session_id , subjcet))\n",
    "        \n",
    "        person = np.concatenate([load_data('../incremental/data/incremental/%s/s%d/%s/%s' % (str_freq , session_id , subjcet , filename)) for filename in filenames] , axis = 0)\n",
    "        \n",
    "        person_data , person_label = separate( person , label = int(subjcet) , overlap_length = overlap_length)\n",
    "        \n",
    "        data.append(person_data)\n",
    "        labels.append(person_label)\n",
    "    \n",
    "    #合并数据\n",
    "    data = np.concatenate(data)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    #shuffle\n",
    "    idx_data = list(range(data.shape[0]))\n",
    "    np.random.shuffle(idx_data)\n",
    "\n",
    "    data = data[idx_data]\n",
    "    labels = labels[idx_data]\n",
    "    \n",
    "    return data , labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA降维\n",
    "## kmeans聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense , Dropout , Conv2D , MaxPooling2D , Reshape , BatchNormalization , Flatten\n",
    "from keras.layers import Input , LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape = (256*3 , 16))\n",
    "\n",
    "#encoder\n",
    "encoder = LSTM(32 , return_sequences=True)(encoder_input)\n",
    "encoder_output = Dense(64 , activation='relu')(encoder)\n",
    "encoder_output = Flatten()(encoder_output)\n",
    "encoder_output = Dense(64 , activation='relu')(encoder_output)\n",
    "\n",
    "#decoder\n",
    "decoder = Dense(768*64 , activation='relu')(encoder_output)\n",
    "decoder = Reshape(target_shape=(768 , 64))(decoder)\n",
    "decoder = LSTM(32)(decoder)\n",
    "decoder_output = Dense(256*3*16 , activation='relu')(decoder)\n",
    "decoder_output = Reshape(target_shape=(256*3 , 16))(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs=encoder_input , outputs=decoder_output)\n",
    "encoder = Model(inputs=encoder_input , outputs=encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意 分两次运行 6 7.5 和 8.5 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq :  6\n",
      "session :  1 [[-103.30456   -42.774944]\n",
      " [ 990.41473     6.174515]\n",
      " [-145.89462   493.09192 ]\n",
      " [-140.4229   -111.534676]\n",
      " [-139.74927   597.19586 ]\n",
      " [-147.90546   346.20505 ]\n",
      " [-150.65353    63.878716]\n",
      " [-156.25842   -23.859875]\n",
      " [-144.73189   -48.534504]] 0.096361\n",
      "session :  3 [[-1.4558879e+02 -1.8367999e+00]\n",
      " [ 1.1654363e+03  2.5297263e-01]\n",
      " [-1.4851292e+02  4.1702633e+01]\n",
      " [-1.4292363e+02 -3.9863430e+01]\n",
      " [-1.4500789e+02 -1.0390284e+01]\n",
      " [-1.4408551e+02 -2.3640768e+01]\n",
      " [-1.4617097e+02  7.2626395e+00]\n",
      " [-1.4800337e+02  3.4528839e+01]\n",
      " [-1.4532034e+02 -5.9245410e+00]] 0.05057059999944613\n",
      "session :  5 [[ 195.77013   -33.556423]\n",
      " [-101.32226   -24.268118]\n",
      " [  29.592646  140.68901 ]\n",
      " [ 167.79861    53.64836 ]\n",
      " [  38.217876   65.95049 ]\n",
      " [ 172.45627   -67.90003 ]\n",
      " [ -34.919937   97.60494 ]\n",
      " [ 247.20178    73.007256]\n",
      " [-108.37393    15.763445]] 0.09629039999890665\n",
      "session :  6 [[-59.345722  -55.152966 ]\n",
      " [495.9683      5.7400436]\n",
      " [-47.148582  135.40564  ]\n",
      " [-92.9052    262.77448  ]\n",
      " [ -7.679823   99.01126  ]\n",
      " [-68.7295    -70.25765  ]\n",
      " [-90.38985   238.80939  ]\n",
      " [-36.12758   115.58442  ]\n",
      " [-63.908085  -63.734272 ]] 0.05444639999950596\n",
      "session :  7 [[ 103.96308    -69.8418   ]\n",
      " [ -74.79355     -6.2854843]\n",
      " [ 222.37436     84.31991  ]\n",
      " [  -5.3782625  -28.305025 ]\n",
      " [-153.36905     37.42836  ]\n",
      " [ -58.75044     58.04484  ]\n",
      " [  29.54709    -49.117672 ]\n",
      " [ -36.622982    -2.4317567]\n",
      " [ -93.54148     68.42724  ]] 0.06345169999985956\n",
      "session :  8 [[ 125.14849   -25.599382]\n",
      " [ -86.658875   36.64852 ]\n",
      " [-115.38588   -96.36439 ]\n",
      " [  70.66299   153.11444 ]\n",
      " [ 313.71777  -115.68052 ]\n",
      " [ -84.422745  -50.476837]\n",
      " [ 118.17568  -129.25055 ]\n",
      " [  76.60613    43.50681 ]\n",
      " [ -76.50678   132.17287 ]] 0.06763279999722727\n",
      "session :  9 [[ -86.81489   -56.951305]\n",
      " [ 727.30664     7.447597]\n",
      " [-110.67018   197.76639 ]\n",
      " [ -85.06314    32.015205]\n",
      " [-103.29826   -25.369171]\n",
      " [-104.79304    40.02343 ]\n",
      " [ -76.436676  -67.39507 ]\n",
      " [ -89.679276  -43.52523 ]\n",
      " [-109.001854  177.34872 ]] 0.05586040000343928\n",
      "session :  11 [[347.79538    11.449092 ]\n",
      " [-62.20547   -39.354633 ]\n",
      " [-69.43903   205.11864  ]\n",
      " [ -1.9736016  11.368688 ]\n",
      " [-63.522034  107.45379  ]\n",
      " [-22.656675  -43.480846 ]\n",
      " [-76.458046  304.5676   ]\n",
      " [-69.851395  -22.809399 ]\n",
      " [ 27.20366   -32.763767 ]] 0.061037400002533104\n",
      "session :  12 [[ -81.11035    -33.29674  ]\n",
      " [ 273.0439     -47.31096  ]\n",
      " [ -69.87598     94.893486 ]\n",
      " [  26.3138     262.2724   ]\n",
      " [ -39.98407   -101.27893  ]\n",
      " [ 139.82507    169.90485  ]\n",
      " [ -34.848305     2.2870684]\n",
      " [ 376.35687    -42.082428 ]\n",
      " [ 203.08206   -110.118614 ]] 0.06193410000560107\n",
      "session :  13 [[-104.508896   211.48416  ]\n",
      " [ -56.164806  -103.87517  ]\n",
      " [ 572.0062      13.204979 ]\n",
      " [-117.16143      2.2832763]\n",
      " [ -20.947529   -20.017149 ]\n",
      " [ -51.785793   183.70935  ]\n",
      " [ -92.06052    -64.099304 ]\n",
      " [-104.78864    106.711136 ]\n",
      " [ -21.537363   -83.14341  ]] 0.059939099999610335\n",
      "freq :  7.5\n",
      "session :  1 [[   4.9624724 -101.11947  ]\n",
      " [-162.02309     55.98423  ]\n",
      " [ 161.33267    160.5054   ]\n",
      " [ 178.32782    -51.661552 ]\n",
      " [  -9.141321    -1.551672 ]\n",
      " [-138.05594    -21.14193  ]\n",
      " [  20.432495   -37.296314 ]\n",
      " [-120.7886      51.695915 ]\n",
      " [  48.586803   -90.17546  ]] 0.06369660000200383\n",
      "session :  3 [[-102.17985    38.358707]\n",
      " [ 760.047      24.26069 ]\n",
      " [ -86.04554   -86.25266 ]\n",
      " [-144.0359    193.02402 ]\n",
      " [  19.067326 -153.98691 ]\n",
      " [-103.59582   -45.551975]\n",
      " [-123.70512    66.77263 ]\n",
      " [ 797.73395    27.934584]\n",
      " [ -35.97404  -129.37027 ]] 0.06629889999749139\n",
      "session :  5 [[-158.61804   -51.743103]\n",
      " [ 882.0348    -50.99104 ]\n",
      " [ 124.77056   155.68097 ]\n",
      " [ -96.576744  110.73948 ]\n",
      " [-151.53871   -18.160643]\n",
      " [-163.8886    -90.27582 ]\n",
      " [-125.71179    92.8295  ]\n",
      " [ 174.83235   153.00984 ]\n",
      " [-141.1089      7.896216]] 0.05923320000147214\n",
      "session :  6 [[-106.92661   -67.88718 ]\n",
      " [1363.5688    -16.614473]\n",
      " [ -57.66606   238.1785  ]\n",
      " [ -76.3354    112.6133  ]\n",
      " [-136.23724    23.153643]\n",
      " [ 754.3561     77.68476 ]\n",
      " [-130.63087   -27.07448 ]\n",
      " [ -90.66728   -47.51186 ]\n",
      " [ 263.832     -34.538826]] 0.05746490000456106\n",
      "session :  7 [[-102.0623     246.87231  ]\n",
      " [ -34.42363    -20.94734  ]\n",
      " [ 335.61557     14.2901745]\n",
      " [ -29.759048   -92.11439  ]\n",
      " [ -82.75552     92.73038  ]\n",
      " [ -54.184982  -154.67354  ]\n",
      " [ 466.09753     50.41497  ]\n",
      " [  32.23299    -17.2347   ]\n",
      " [-101.43714    -12.274932 ]] 0.06266149999282788\n",
      "session :  8 [[-185.2526    -84.707085]\n",
      " [1371.6786      5.671557]\n",
      " [-192.02248   115.20924 ]\n",
      " [-131.01823  -132.78644 ]\n",
      " [-171.06964    61.3607  ]\n",
      " [-159.25272   184.053   ]\n",
      " [-150.08641   -16.006142]\n",
      " [-224.20518    39.293667]\n",
      " [-158.77133  -172.08891 ]] 0.0535132000077283\n",
      "session :  9 [[-153.3394     53.739983]\n",
      " [ 118.09068   -79.4539  ]\n",
      " [ 286.62552   131.46545 ]\n",
      " [ -22.333704  -52.557236]\n",
      " [ -99.81223     8.073117]\n",
      " [ -72.889595  -12.675421]\n",
      " [  68.70257   -72.0747  ]\n",
      " [  29.99378   -66.60153 ]\n",
      " [-131.6114     40.579166]] 0.06204770000476856\n",
      "session :  11 [[ -85.015785   39.779327]\n",
      " [ 313.41498   -87.38546 ]\n",
      " [  96.38142  -251.59009 ]\n",
      " [ 607.7692    265.66632 ]\n",
      " [ 317.02692   170.15788 ]\n",
      " [-173.29504    77.73289 ]\n",
      " [ -41.409595  -59.841114]\n",
      " [ 136.8523    113.48909 ]\n",
      " [-147.88385     9.313354]] 0.05766030000813771\n",
      "session :  12 [[1307.6315      10.925044 ]\n",
      " [-186.58902     -5.4031544]\n",
      " [-182.51854     90.65106  ]\n",
      " [-137.53421    -64.873924 ]\n",
      " [-144.41264    123.27157  ]\n",
      " [1251.0802      -2.5444176]\n",
      " [-227.75723     65.54734  ]\n",
      " [-132.085      -25.827763 ]\n",
      " [-145.1759     -80.215355 ]] 0.07650760000979062\n",
      "session :  13 [[ -89.12564   133.01476 ]\n",
      " [ 102.60074   -51.557377]\n",
      " [ 429.70718    34.094044]\n",
      " [ -94.55388   -39.82819 ]\n",
      " [ -10.953704  -86.95393 ]\n",
      " [-103.51335    13.793837]\n",
      " [ -62.884697  -91.02127 ]\n",
      " [ -70.598114   77.203575]\n",
      " [ 160.70055   -53.934376]] 0.05875409999862313\n",
      "freq :  8.5\n",
      "session :  1 [[-1.08658142e+02  1.81613040e+00]\n",
      " [ 8.27877869e+02  8.00619984e+00]\n",
      " [-1.37851593e+02  1.44892181e+02]\n",
      " [-7.35940704e+01 -5.79517670e+01]\n",
      " [-1.22691765e+02  7.74143829e+01]\n",
      " [ 9.55905090e+02  2.26888580e+01]\n",
      " [-1.07007812e+02 -3.91378479e+01]\n",
      " [ 7.52084839e+02 -4.69139457e-01]\n",
      " [ 8.95186279e+02  1.57719221e+01]] 0.057522699993569404\n",
      "session :  3 [[-132.68538    -25.514423 ]\n",
      " [1041.3035      -4.551676 ]\n",
      " [ -73.5833      99.03397  ]\n",
      " [ 905.475       -6.260728 ]\n",
      " [-116.80182     28.882433 ]\n",
      " [1097.969       -3.8629146]\n",
      " [ 976.0021      -5.7765503]\n",
      " [-118.99191    -40.498867 ]\n",
      " [-136.91045    -10.350238 ]] 0.055908899987116456\n",
      "session :  5 [[  3.8702538 -15.9685755]\n",
      " [ 92.28568    39.535572 ]\n",
      " [-61.36702    50.922043 ]\n",
      " [-21.743572   -2.5295105]\n",
      " [-11.029842  -11.273633 ]\n",
      " [ 30.417707  -13.67703  ]\n",
      " [  9.153726  -33.99425  ]\n",
      " [-13.82333   -19.580782 ]\n",
      " [-35.50534     9.3090515]] 0.06907040002988651\n",
      "session :  6 [[-124.09382    -99.71345  ]\n",
      " [1173.2014       4.0358677]\n",
      " [-167.68504     92.24843  ]\n",
      " [-160.98654    -21.694613 ]\n",
      " [-137.69804    132.80173  ]\n",
      " [-146.54141    -46.646255 ]\n",
      " [-184.15323     63.32165  ]\n",
      " [-135.6907     -70.27397  ]\n",
      " [1239.7067       5.4521623]] 0.059553900005994365\n",
      "session :  7 [[-10.353027  -20.70103  ]\n",
      " [ 58.903133    8.7442875]\n",
      " [-67.06465    52.357403 ]\n",
      " [-35.31012   -16.942923 ]\n",
      " [ -2.5006473  -9.965709 ]\n",
      " [ 68.09997    25.979605 ]\n",
      " [ 84.04116     9.176803 ]\n",
      " [ 27.500126  -10.969305 ]\n",
      " [-73.53426    66.19351  ]] 0.0636518000101205\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,768,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_2/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](reshape_1/Reshape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_25/mul/_3577 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3501_loss_25/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d64790603a63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#自编码器的训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#encoder部分进行预测输出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdata_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#输出2维新特征 在xOy坐标系绘制\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,768,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_2/zeros_like = ZerosLike[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](reshape_1/Reshape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss_25/mul/_3577 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3501_loss_25/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "for freq in [6 , 7.5]:\n",
    "    \n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        \n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #自编码器的训练\n",
    "        autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "        autoencoder.fit(data , data , batch_size=32 , epochs=50 , verbose=0)\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_pca = pca.fit_transform(data_feature)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=9)\n",
    "        start = time.clock()\n",
    "        _ = kmeans.fit_transform(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        print('session : ' , session_id , kmeans.cluster_centers_ , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq :  8.5\n"
     ]
    }
   ],
   "source": [
    "for freq in [8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_pca = pca.fit_transform(data_feature)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=9)\n",
    "        start = time.clock()\n",
    "        _ = kmeans.fit_transform(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        print('session : ' , session_id , kmeans.cluster_centers_ , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_pca = pca.fit_transform(data_feature)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        dbscan = DBSCAN()\n",
    "        start = time.clock()\n",
    "        res = dbscan.fit_predict(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        _centers = get_center(X_pca , res)\n",
    "        \n",
    "        print('session : ' , session_id , _centers , time1 + time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_pca = pca.fit_transform(data_feature)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=9 , tol=0.001)\n",
    "        start = time.clock()\n",
    "        _ = kmeans.fit_transform(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        print('session : ' , session_id , kmeans.cluster_centers_ , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_pca = pca.fit_transform(data_feature)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=9)\n",
    "        start = time.clock()\n",
    "        res = gmm.fit_predict(X_lda)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        _centers = get_center(X_lda , res)\n",
    "        \n",
    "        print('session : ' , session_id , _centers , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA降维\n",
    "## kmeans聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_lda = lda.fit_transform(X_sbp , y)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=9)\n",
    "        start = time.clock()\n",
    "        _ = kmeans.fit_transform(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        print('session : ' , session_id , kmeans.cluster_centers_ , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_lda = lda.fit_transform(X_sbp , y)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        dbscan = DBSCAN()\n",
    "        start = time.clock()\n",
    "        res = dbscan.fit_predict(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        _centers = get_center(X_pca , res)\n",
    "        \n",
    "        print('session : ' , session_id , _centers , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_lda = lda.fit_transform(X_sbp , y)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=9 , tol=0.001)\n",
    "        start = time.clock()\n",
    "        _ = kmeans.fit_transform(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        print('session : ' , session_id , kmeans.cluster_centers_ , time1 + time2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for freq in [6 , 7.5 , 8.5 , 10]:\n",
    "    print('freq : ' , freq)\n",
    "    \n",
    "    meta_data , meta_labels = combine(freq)\n",
    "    meta_data = (meta_data-np.mean(meta_data)) / np.std(meta_data)\n",
    "    \n",
    "    #自编码器的训练\n",
    "    autoencoder.compile(optimizer='adam' , loss = 'mse')\n",
    "    autoencoder.fit(meta_data , meta_data , batch_size=32 , epochs=50 , verbose=0)\n",
    "    \n",
    "    for session_id in [1 , 3 , 5 , 6 , 7 , 8 , 9 , 11 , 12 , 13]:\n",
    "        data , labels = session_data_labels(session_id , freq , is_training=True)\n",
    "        data = (data-np.mean(data)) / np.std(data)\n",
    "\n",
    "        #encoder部分进行预测输出\n",
    "        data_feature = encoder.predict(x = data) #输出2维新特征 在xOy坐标系绘制\n",
    "        \n",
    "        lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "        start = time.clock()\n",
    "        X_lda = lda.fit_transform(X_sbp , y)\n",
    "        time1 = time.clock() - start\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=9)\n",
    "        start = time.clock()\n",
    "        res = gmm.fit_predict(X_pca)\n",
    "        time2 = time.clock() - start\n",
    "        \n",
    "        _centers = get_center(X_pca , res)\n",
    "        \n",
    "        print('session : ' , session_id , _centers , time1 + time2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
