{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = 128 #hz\n",
    "trial_time = 3 #s\n",
    "\n",
    "origin_channel = 5 #5 channel eeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_mat(X):\n",
    "    return np.matmul(X , X.T)/np.trace(np.matmul(X , X.T))\n",
    "\n",
    "#计算每种样本的平均协方差矩阵\n",
    "def average_norm_cov_mat(data):\n",
    "    count = data.shape[0]\n",
    "    sum_mat = np.zeros(shape=(data[0].shape[0] , data[0].shape[0]))\n",
    "    \n",
    "    for i in range(count):\n",
    "        sum_mat += cov_mat(data[i])\n",
    "    \n",
    "    return sum_mat/count\n",
    "\n",
    "def load_data(file_name):\n",
    "    #pink and white\n",
    "    \n",
    "    temp = pd.read_csv(file_name)\n",
    "    \n",
    "    #删除前3秒和后2秒数据\n",
    "    temp = temp.iloc[ : temp.shape[0] - 2*128] #后2秒 2s sample:128hz\n",
    "    temp = temp.iloc[3*128 : ] #前3秒 3s sample:128hz\n",
    "    \n",
    "    for column in temp.columns:\n",
    "        temp[column] = (temp[column] - temp[column].mean())/temp[column].std() #norm\n",
    "    \n",
    "    #5 channels data\n",
    "    return temp[['AF3' , 'T7','Pz' , 'T8' , 'AF4']]\n",
    "\n",
    "def sep(one_data , label):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    \n",
    "    size = sample*trial_time #384\n",
    "    \n",
    "    for i in range(one_data.shape[0] - size):\n",
    "        train_data.append(one_data.iloc[i : i+size].values) #add one train sample\n",
    "        train_labels.append(label) #corresponding label\n",
    "    \n",
    "    return train_data , train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_eeg_csv(file_names):\n",
    "    #concat a big csv file\n",
    "    first_file = load_data(file_name = file_names[0])\n",
    "    \n",
    "    file_names.remove(file_names[0])\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        first_file = first_file.append(load_data(file_name = file_name) , ignore_index = True)\n",
    "    \n",
    "    return first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#在进行聚类 坐标系绘图时 将m设置为1 进行绘制 CSP算法后特征变为2维\n",
    "\n",
    "m = 2 #选取最高m个特征向量 m最小为1\n",
    "def CSP_matrix(train_data_1 , train_data_2):\n",
    "    #计算投影矩阵\n",
    "    R_pink = average_norm_cov_mat(train_data_1)\n",
    "    R_white = average_norm_cov_mat(train_data_2)\n",
    "    \n",
    "    R = R_pink + R_white\n",
    "    \n",
    "    eigenvalues , U0 = np.linalg.eig(R)\n",
    "\n",
    "    sort_index = np.argsort(eigenvalues)\n",
    "    sort_index = sort_index[:: -1]\n",
    "\n",
    "    U0 = U0[: , sort_index]\n",
    "    eigenvalues = sorted(eigenvalues , reverse=True)\n",
    "\n",
    "    sigma = np.diag(eigenvalues)\n",
    "    \n",
    "    #白化矩阵\n",
    "    P = np.matmul(np.diag(np.power(eigenvalues , -0.5)) , U0.T)\n",
    "    \n",
    "    S_pink = np.matmul(np.matmul(P , R_pink) , P.T)\n",
    "    S_white = np.matmul(np.matmul(P , R_white) , P.T)\n",
    "    \n",
    "    E1 , US1 = np.linalg.eig(S_pink)\n",
    "    E2 , US2 = np.linalg.eig(S_white)\n",
    "\n",
    "    #E1+E2=I\n",
    "    #US1=US2\n",
    "    \n",
    "    sort_index_1 = np.argsort(E1)\n",
    "    sort_index_1 = sort_index_1[:: -1]\n",
    "    \n",
    "    E1 = sorted(E1 , reverse=True)\n",
    "    US1 = US1[: , sort_index_1]\n",
    "    \n",
    "    W_1 = np.matmul(US1[: , 0:m].T , P) #前2列特征向量\n",
    "    #=======================\n",
    "    sort_index_2 = np.argsort(E2)\n",
    "    sort_index_2 = sort_index_2[:: -1]\n",
    "    \n",
    "    E2 = sorted(E2 , reverse=True)\n",
    "    US2 = US2[: , sort_index_2]\n",
    "    \n",
    "    W_2 = np.matmul(US2[: , 0:m].T , P) #前2列特征向量\n",
    "    \n",
    "    return W_1 , W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#low pass filter\n",
    "#50Hz\n",
    "\n",
    "def low_pass(data):\n",
    "    point = 50 #highest freq = 50hz\n",
    "    length = sample * trial_time #256\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            a = np.fft.fft(data[i][j]).real\n",
    "            b = np.fft.fft(data[i][j]).imag\n",
    "            a[point : length-point] = 0\n",
    "            b[point : length-point] = 0\n",
    "            #重建频谱\n",
    "            new_freq = [np.complex(a[i] , b[i]) for i in range(length)]\n",
    "            new_freq = np.array(new_freq)\n",
    "            \n",
    "            data[i][j] = np.fft.ifft(new_freq)\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用白色\n",
    "#data_1 = concat_eeg_csv(['data/train_1/fei_white_1.csv' , 'data/train_1/fei_white_2.csv'])\n",
    "#data_2 = concat_eeg_csv(['data/train_1/sen_white_1.csv' , 'data/train_1/sen_white_2.csv'])\n",
    "\n",
    "#使用粉色\n",
    "data_1 = concat_eeg_csv(['data/train_1/fei_pink_1.csv' , 'data/train_1/fei_pink_2.csv'])\n",
    "data_2 = concat_eeg_csv(['data/train_1/sen_pink_1.csv' , 'data/train_1/sen_pink_2.csv'])\n",
    "\n",
    "train_data_1 , train_labels_1 = sep(data_1 , 0)\n",
    "train_data_2 , train_labels_2 = sep(data_2 , 1)\n",
    "\n",
    "train_data_1 = np.array(train_data_1)\n",
    "train_data_2 = np.array(train_data_2)\n",
    "\n",
    "train_labels_1 = np.array(train_labels_1)\n",
    "train_labels_2 = np.array(train_labels_2)\n",
    "\n",
    "train_data_1 = np.transpose(train_data_1 , axes=(0 , 2 , 1))\n",
    "train_data_2 = np.transpose(train_data_2 , axes=(0 , 2 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8752, 5, 384) (24762, 5, 384)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_1.shape , train_data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "#=============\n",
    "#滤波阶段 此时一个小样本为3秒的数据量  此时大致认为信号为平稳的\n",
    "train_data_1 = low_pass(train_data_1)\n",
    "train_data_2 = low_pass(train_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_1 , W_2 = CSP_matrix(train_data_1 , train_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74196145,  1.27667659,  0.20311701, -0.83116758, -0.02975433],\n",
       "       [-1.18976692,  0.70299699, -1.50908085,  1.4120881 , -0.08062964]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77692374, -0.68865527, -0.0326544 , -0.44722146,  1.06682074],\n",
       "       [-1.18801686,  0.14026237,  1.0596426 ,  0.94656048,  0.51055565]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_data_1 , train_data_2))\n",
    "\n",
    "train_labels = np.concatenate((train_labels_1 , train_labels_2))\n",
    "\n",
    "train_data_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_feature(xi):\n",
    "    #m = 2 #2*m features\n",
    "    #\n",
    "    #Z = np.matmul(W , xi)\n",
    "    #\n",
    "    #Z_pre_m = Z[0:m ,:]\n",
    "    #Z_after_m = Z[-m: , :]\n",
    "    #\n",
    "    #Zp = np.concatenate((Z_pre_m , Z_after_m))\n",
    "    #\n",
    "    #sigma_var_Zp = 0\n",
    "    #\n",
    "    #for i in range(2*m):\n",
    "    #    sigma_var_Zp += np.var(Zp[i])\n",
    "    #    \n",
    "    #fi = [np.log(np.var(Zpi)/sigma_var_Zp) for Zpi in Zp]\n",
    "    #\n",
    "    #fi = np.array(fi)\n",
    "    \n",
    "    #======\n",
    "    #特征维数较低时 例如低于100\n",
    "    #fi = np.concatenate([np.matmul(W_1 , xi) , np.matmul(W_2 , xi)])\n",
    "    #======\n",
    "    \n",
    "    \n",
    "    '''特征提取方式1'''\n",
    "    z1 = np.matmul(W_1 , xi) #2*384\n",
    "    z2 = np.matmul(W_2 , xi) #2*384\n",
    "    var_z1_1 = np.var(z1 , axis = 1)[0]\n",
    "    var_z1_2 = np.var(z1 , axis = 1)[1]\n",
    "    var_z2_1 = np.var(z2 , axis = 1)[0]\n",
    "    var_z2_2 = np.var(z2 , axis = 1)[1]\n",
    "    \n",
    "    f1 = var_z1_1/(var_z1_1+var_z2_1)\n",
    "    f2 = var_z2_1/(var_z1_1+var_z2_1)\n",
    "    f3 = var_z1_2/(var_z1_2+var_z2_2)\n",
    "    f4 = var_z2_2/(var_z1_2+var_z2_2)\n",
    "    \n",
    "    return np.array([f1 , f2 , f3 , f4])\n",
    "    #==========\n",
    "    \n",
    "    '''特征提取方式2'''\n",
    "    #z1 = np.matmul(W_1 , xi) #2*384\n",
    "    #z2 = np.matmul(W_2 , xi) #2*384\n",
    "    #return np.concatenate((z1 , z2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(train_data.shape[0]):\n",
    "    train_data_features.append(fetch_feature(train_data[i]))\n",
    "\n",
    "train_data_features = np.array(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_data_features = np.transpose(train_data_features , axes=(0 , 2 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33514, 4) (33514,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features.shape , train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(verbose = True)\n",
    "svc.fit(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988959837679776"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(verbose=False)\n",
    "\n",
    "rf.fit(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost.score(train_data_features , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============\n",
    "#==============\n",
    "#val step\n",
    "#白色\n",
    "#data_1_val = load_data('data/val_1/fei_white_3.csv')\n",
    "#data_2_val = load_data('data/val_1/sen_white_3.csv')\n",
    "\n",
    "#粉色\n",
    "data_1_val = load_data('data/val_1/fei_pink_3.csv')\n",
    "data_2_val = load_data('data/val_1/sen_pink_3.csv')\n",
    "\n",
    "val_data_1 , val_labels_1 = sep(data_1_val , 0)\n",
    "val_data_2 , val_labels_2 = sep(data_2_val , 1)\n",
    "\n",
    "val_data_1 = np.array(val_data_1)\n",
    "val_data_2 = np.array(val_data_2)\n",
    "\n",
    "val_labels_1 = np.array(val_labels_1)\n",
    "val_labels_2 = np.array(val_labels_2)\n",
    "\n",
    "val_data_1 = np.transpose(val_data_1 , axes=(0 , 2 , 1))\n",
    "val_data_2 = np.transpose(val_data_2 , axes=(0 , 2 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "#================\n",
    "#验证集滤波\n",
    "val_data_1 = low_pass(val_data_1)\n",
    "val_data_2 = low_pass(val_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===================\n",
    "#concat\n",
    "val_data_features = []\n",
    "\n",
    "val_data = np.concatenate((val_data_1 , val_data_2))\n",
    "val_labels = np.concatenate((val_labels_1 , val_labels_2))\n",
    "\n",
    "\n",
    "#===================\n",
    "#fetch feature\n",
    "for i in range(val_data.shape[0]):\n",
    "    val_data_features.append(fetch_feature(val_data[i]))\n",
    "\n",
    "val_data_features = np.array(val_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16507, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9207003089598352"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "svc.score(val_data_features , val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9216695947173926"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RandomForest\n",
    "rf.score(val_data_features , val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050705761192221"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost.score(val_data_features , val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#使用CNN进行处理\n",
    "#最后的输出设置为1个输出单元 激活为sigmoid\n",
    "#========\n",
    "#========\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout , Conv2D , MaxPooling2D , Reshape , BatchNormalization , Flatten\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "#sample*trial_time=384\n",
    "#网络输入为384维\n",
    "\n",
    "#CNNs需要的输入的样本集 形式为（样本数量，384,4）4为新的特征数量 5为原始特征数量\n",
    "\n",
    "model.add(Dense(units=30 , input_shape=(sample*trial_time , 4) , activation='elu' , kernel_regularizer=l2()))\n",
    "model.add(Reshape((30 , sample*trial_time , 1)))\n",
    "\n",
    "#conv pool 1\n",
    "model.add(Conv2D(60 , kernel_size=(1,15) , strides=(1,3) , padding='valid' , activation='elu' , kernel_regularizer=l2()))\n",
    "model.add(MaxPooling2D(pool_size=(1,2) , strides=(1,2) , padding='valid'))\n",
    "\n",
    "#dropout\n",
    "model.add(Dropout(1 - keep_prob))\n",
    "#batch norm\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv pool 2\n",
    "model.add(Conv2D(60 , kernel_size=(1,4) , strides=(1,3) , padding='valid' , activation='elu' , kernel_regularizer=l2()))\n",
    "model.add(MaxPooling2D(pool_size=(1,2) , strides=(1,2) , padding='valid'))\n",
    "#dropout\n",
    "model.add(Dropout(1 - keep_prob))\n",
    "#batch norm\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv 3\n",
    "model.add(Conv2D(60 , kernel_size=(30,1) , strides=(1,3) , padding='valid' , activation='elu' , kernel_regularizer=l2()))\n",
    "#dropout\n",
    "model.add(Dropout(1 - keep_prob))\n",
    "#batch norm\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv pool 4\n",
    "model.add(Conv2D(90 , kernel_size=(1,3) , strides=(1,1) , padding='same' , activation='elu' , kernel_regularizer=l2()))\n",
    "model.add(MaxPooling2D(pool_size=(1,2) , strides=(1,2) , padding='valid'))\n",
    "#dropout\n",
    "model.add(Dropout(1 - keep_prob))\n",
    "#batch norm\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#conv pool 5\n",
    "model.add(Conv2D(120 , kernel_size=(1,3) , strides=(1,1) , padding='same' , activation='elu' , kernel_regularizer=l2()))\n",
    "model.add(MaxPooling2D(pool_size=(1,2) , strides=(1,2) , padding='valid'))\n",
    "#dropout\n",
    "model.add(Dropout(1 - keep_prob))\n",
    "#batch norm\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "#fc\n",
    "model.add(Dense(units=24 , activation='elu' , kernel_regularizer=l2()))\n",
    "\n",
    "#extra for cluster layer\n",
    "#model.add(Dense(units=12 , activation='elu' , kernel_regularizer=l2() , name = 'feature1'))\n",
    "#model.add(Dense(units=4 , activation='elu' , kernel_regularizer=l2() , name = 'feature2'))\n",
    "#model.add(Dense(units=2 ,  activation='elu' , kernel_regularizer=l2() , name = 'feature3'))\n",
    "\n",
    "#fc last layer\n",
    "model.add(Dense(units=2 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 384, 30)           150       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 30, 384, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 124, 60)       960       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 62, 60)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 62, 60)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 62, 60)        240       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 20, 60)        14460     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 10, 60)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 10, 60)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 10, 60)        240       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 4, 60)          108060    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 4, 60)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1, 4, 60)          240       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 4, 90)          16290     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 2, 90)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 2, 90)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 2, 90)          360       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 2, 120)         32520     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 120)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 120)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 120)         480       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                2904      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 176,954\n",
      "Trainable params: 176,174\n",
      "Non-trainable params: 780\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d2b30c8240c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_data_features , train_labels , batch_size=16 , epochs=100 , shuffle=True , validation_data=(val_data_features , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
