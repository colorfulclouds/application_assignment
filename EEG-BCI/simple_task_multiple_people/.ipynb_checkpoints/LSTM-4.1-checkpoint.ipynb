{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use LSTM to classify EEG filter and no filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = 128 #hz\n",
    "trial_time = 3 #s\n",
    "\n",
    "origin_channel = 5 #5 channel eeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_mat(X):\n",
    "    return np.matmul(X , X.T)/np.trace(np.matmul(X , X.T))\n",
    "\n",
    "#计算每种样本的平均协方差矩阵\n",
    "def average_norm_cov_mat(data):\n",
    "    count = data.shape[0]\n",
    "    sum_mat = np.zeros(shape=(data[0].shape[0] , data[0].shape[0]))\n",
    "    \n",
    "    for i in range(count):\n",
    "        sum_mat += cov_mat(data[i])\n",
    "    \n",
    "    return sum_mat/count\n",
    "\n",
    "def load_data(file_name):\n",
    "    #pink and white\n",
    "    \n",
    "    temp = pd.read_csv(file_name)\n",
    "    \n",
    "    #删除前3秒和后2秒数据\n",
    "    temp = temp.iloc[ : temp.shape[0] - 2*128] #后2秒 2s sample:128hz\n",
    "    temp = temp.iloc[3*128 : ] #前3秒 3s sample:128hz\n",
    "    \n",
    "    for column in temp.columns:\n",
    "        temp[column] = (temp[column] - temp[column].mean())/temp[column].std() #norm\n",
    "    \n",
    "    #5 channels data\n",
    "    return temp[['AF3' , 'T7','Pz' , 'T8' , 'AF4']]\n",
    "\n",
    "def sep(one_data , label):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    \n",
    "    size = sample*trial_time #384\n",
    "    \n",
    "    for i in range(one_data.shape[0] - size):\n",
    "        train_data.append(one_data.iloc[i : i+size].values) #add one train sample\n",
    "        train_labels.append(label) #corresponding label\n",
    "    \n",
    "    return train_data , train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_eeg_csv(file_names):\n",
    "    #concat a big csv file\n",
    "    first_file = load_data(file_name = file_names[0])\n",
    "    \n",
    "    file_names.remove(file_names[0])\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        first_file = first_file.append(load_data(file_name = file_name) , ignore_index = True)\n",
    "    \n",
    "    return first_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#low pass filter\n",
    "#50Hz\n",
    "\n",
    "def low_pass(data):\n",
    "    point = 50 #highest freq = 50hz\n",
    "    length = sample * trial_time #256\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            a = np.fft.fft(data[i][j]).real\n",
    "            b = np.fft.fft(data[i][j]).imag\n",
    "            a[point : length-point] = 0\n",
    "            b[point : length-point] = 0\n",
    "            #重建频谱\n",
    "            new_freq = [np.complex(a[i] , b[i]) for i in range(length)]\n",
    "            new_freq = np.array(new_freq)\n",
    "            \n",
    "            data[i][j] = np.fft.ifft(new_freq)\n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用白色\n",
    "#data_1 = concat_eeg_csv(['data/train_1/fei_white_1.csv' , 'data/train_1/fei_white_2.csv'])\n",
    "#data_2 = concat_eeg_csv(['data/train_1/sen_white_1.csv' , 'data/train_1/sen_white_2.csv'])\n",
    "\n",
    "#使用粉色\n",
    "data_1 = concat_eeg_csv(['data/train_1/fei_pink_1.csv' , 'data/train_1/fei_pink_2.csv'])\n",
    "data_2 = concat_eeg_csv(['data/train_1/sen_pink_1.csv' , 'data/train_1/sen_pink_2.csv'])\n",
    "\n",
    "train_data_1 , train_labels_1 = sep(data_1 , 0)\n",
    "train_data_2 , train_labels_2 = sep(data_2 , 1)\n",
    "\n",
    "train_data_1 = np.array(train_data_1)\n",
    "train_data_2 = np.array(train_data_2)\n",
    "\n",
    "train_labels_1 = np.array(train_labels_1)\n",
    "train_labels_2 = np.array(train_labels_2)\n",
    "\n",
    "train_data_1 = np.transpose(train_data_1 , axes=(0 , 2 , 1))\n",
    "train_data_2 = np.transpose(train_data_2 , axes=(0 , 2 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============\n",
    "#滤波阶段 此时一个小样本为3秒的数据量  此时大致认为信号为平稳的\n",
    "#train_data_1 = low_pass(train_data_1)\n",
    "#train_data_2 = low_pass(train_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_data_1 , train_data_2))\n",
    "\n",
    "train_labels = np.concatenate((train_labels_1 , train_labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.transpose(train_data , axes = (0 , 2 , 1))\n",
    "train_labels = np_utils.to_categorical(train_labels , num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM , Dense\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=32 , input_shape=(sample*trial_time , origin_channel) , return_sequences=False))\n",
    "model.add(Dense(units=8 , activation='tanh'))\n",
    "model.add(Dense(units=2 , activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.002) , loss='categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============\n",
    "#==============\n",
    "#val step\n",
    "#白色\n",
    "#data_1_val = load_data('data/val_1/fei_white_3.csv')\n",
    "#data_2_val = load_data('data/val_1/sen_white_3.csv')\n",
    "\n",
    "#粉色\n",
    "data_1_val = load_data('data/val_1/fei_pink_3.csv')\n",
    "data_2_val = load_data('data/val_1/sen_pink_3.csv')\n",
    "\n",
    "val_data_1 , val_labels_1 = sep(data_1_val , 0)\n",
    "val_data_2 , val_labels_2 = sep(data_2_val , 1)\n",
    "\n",
    "val_data_1 = np.array(val_data_1)\n",
    "val_data_2 = np.array(val_data_2)\n",
    "\n",
    "val_labels_1 = np.array(val_labels_1)\n",
    "val_labels_2 = np.array(val_labels_2)\n",
    "\n",
    "val_data_1 = np.transpose(val_data_1 , axes=(0 , 2 , 1))\n",
    "val_data_2 = np.transpose(val_data_2 , axes=(0 , 2 , 1))\n",
    "#===============\n",
    "#==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================\n",
    "#验证集滤波\n",
    "#val_data_1 = low_pass(val_data_1)\n",
    "#val_data_2 = low_pass(val_data_2)\n",
    "#================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================\n",
    "val_data = np.concatenate((val_data_1 , val_data_2))\n",
    "val_labels = np.concatenate((val_labels_1 , val_labels_2))\n",
    "#================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================\n",
    "val_data = np.transpose(val_data , axes = (0 , 2 , 1))\n",
    "val_labels = np_utils.to_categorical(val_labels , num_classes=2)\n",
    "#================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33514 samples, validate on 16507 samples\n",
      "Epoch 1/1\n",
      "33514/33514 [==============================] - 562s 17ms/step - loss: 0.0894 - acc: 0.9729 - val_loss: 0.5472 - val_acc: 0.9018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff64e65c88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data , train_labels , batch_size=32 , epochs=1 , validation_data=(val_data , val_labels) , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_data , val_labels , batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
